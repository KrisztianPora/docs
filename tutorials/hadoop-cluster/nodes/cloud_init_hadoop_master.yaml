#cloud-config
package_upgrade: false


write_files:

- path: /tmp/installation.sh
  content: |
    #!/bin/bash

    set -ex
    HADOOP_VERSION={{variables.HADOOP_VERSION}}
    CONSUL_VERSION={{variables.CONSUL_VERSION}}
    CONSUL_TEMPLATE_VERSION={{variables.CONSUL_TEMPLATE_VERSION}}

    echo "Creating HDUSER starts."
    adduser --disabled-password --gecos "" hduser
    chown -R hduser:hduser /home/hduser
    echo "Creating HDUSER finished."


    hostnamectl set-hostname hadoop-master


    # Turn off unattended upgrade
    sed -i 's/APT::Periodic::Unattended-Upgrade "1";/APT::Periodic::Unattended-Upgrade "0";/g' /etc/apt/apt.conf.d/20auto-upgrades


    echo "Install requirement packages starts."
    # Wait for unattended upgrade
    while [[ `ps aufx | grep -v "grep" | grep "apt.systemd.daily" | wc -l` -gt 0 ]]; do
      echo "The unattended-upgrades are running..."
      sleep 1
    done

    export DEBIAN_FRONTEND=noninteractive
    apt-get update
    apt-get install -y openjdk-8-jdk openjdk-8-jre python3-pip unzip
    echo "Install requirement packages starts."


    echo "Install HADOOP starts."
    wget -nc https://archive.apache.org/dist/hadoop/common/hadoop-$HADOOP_VERSION/hadoop-$HADOOP_VERSION.tar.gz -O /home/hduser/hadoop-$HADOOP_VERSION.tar.gz
    tar -xzf /home/hduser/hadoop-$HADOOP_VERSION.tar.gz --directory /home/hduser
    mkdir /home/hduser/hadoop
    mv /home/hduser/hadoop-$HADOOP_VERSION/* /home/hduser/hadoop
    rm -r /home/hduser/hadoop-$HADOOP_VERSION.tar.gz /home/hduser/hadoop-$HADOOP_VERSION
    echo "Install HADOOP finished."


    echo "Install CONSUL starts."
    wget -nc "https://releases.hashicorp.com/consul/"$CONSUL_VERSION"/consul_"$CONSUL_VERSION"_linux_amd64.zip" -O /home/hduser/consul_"$CONSUL_VERSION"_linux_amd64.zip
    unzip -q /home/hduser/consul_"$CONSUL_VERSION"_linux_amd64.zip -d /home/hduser/consul/
    wget -nc "https://releases.hashicorp.com/consul-template/"$CONSUL_TEMPLATE_VERSION"/consul-template_"$CONSUL_TEMPLATE_VERSION"_linux_amd64.zip" -O /home/hduser/consul-template_"$CONSUL_TEMPLATE_VERSION"_linux_amd64.zip
    unzip -q /home/hduser/consul-template_"$CONSUL_TEMPLATE_VERSION"_linux_amd64.zip -d /home/hduser/consul/
    rm /home/hduser/consul_"$CONSUL_VERSION"_linux_amd64.zip /home/hduser/consul-template_"$CONSUL_TEMPLATE_VERSION"_linux_amd64.zip
    echo "Install CONSUL finished."


    echo -e "####################
    \e[92mInstallation DONE!!!\e[39m
    ####################"
  permissions: '755'

- path: /tmp/configuration.sh
  content: |
    #!/bin/bash

    set -ex
    MASTERIP=`hostname -I | col1`
    HOSTNAME=`hostname -s`


    echo "Configure HADOOP starts."
    touch /home/hduser/.bashrc
    chown hduser:hduser /home/hduser/.bashrc
    echo export PATH="/home/hduser/hadoop/bin:$PATH" >> /home/hduser/.bashrc
    mv /tmp/hadoop/configs/* /home/hduser/hadoop/etc/hadoop
    mv /tmp/hadoop/webconfigs/* /home/hduser/hadoop/share/hadoop/hdfs/webapps/hdfs/WEB-INF/
    echo "hadoop: lpds, admin" >> /home/hduser/hadoop/etc/hadoop/realm.properties
    echo "export JAVA_HOME=/usr/lib/jvm/java-8-openjdk-amd64/jre" >> /home/hduser/hadoop/etc/hadoop/hadoop-env.sh
    echo "export HADOOP_PID_DIR=/home/hduser/hadoop" >> /home/hduser/hadoop/etc/hadoop/hadoop-env.sh
    echo "export HADOOP_LOG_DIR=/home/hduser/hadoop/logs" >> /home/hduser/hadoop/etc/hadoop/hadoop-env.sh
    echo "export HADOOP_CLASSPATH=/usr/lib/jvm/java-8-openjdk-amd64/lib/tools.jar" >> /home/hduser/hadoop/etc/hadoop/hadoop-env.sh
    echo "export JAVA_HOME=/usr/lib/jvm/java-8-openjdk-amd64/jre" >> /home/hduser/.bashrc
    echo "export HADOOP_PID_DIR=/home/hduser/hadoop" >> /home/hduser/.bashrc
    echo "export HADOOP_LOG_DIR=/home/hduser/hadoop/logs" >> /home/hduser/.bashrc
    echo "export HADOOP_CLASSPATH=/usr/lib/jvm/java-8-openjdk-amd64/lib/tools.jar" >> /home/hduser/.bashrc
    mkdir /home/hduser/hadoop/logs
    sed -i 's/HadoopMaster/'$MASTERIP'/g' /home/hduser/hadoop/etc/hadoop/core-site.xml
    sed -i 's/HadoopMaster/'$MASTERIP'/g' /home/hduser/hadoop/etc/hadoop/hdfs-site.xml
    touch /home/hduser/hadoop/etc/hadoop/dfs.exclude
    echo "$MASTERIP $HOSTNAME" >> /etc/hosts
    chown -R hduser:hduser /home/hduser/hadoop
    echo "Configure HADOOP finished."


    su - hduser -c 'mkdir /home/hduser/consul/logs'
    su - hduser -c 'mkdir /home/hduser/consul/data'
    touch /home/hduser/downscale.log
    chown hduser:hduser /home/hduser/downscale.log


    echo "Launch CONSUL starts."
    systemctl start consul.service
    systemctl start consul-template-hosts.service
    echo "Launch CONSUL finished."


    chmod +x /tmp/downscale.sh
    while [[ `cat /etc/hosts | grep 'Consul' | wc -l` -eq 0 ]]; do
      echo "Waiting for /etc/host modification..."
      sleep 1
    done


    echo -e "#####################
    \e[92mConfiguration DONE!!!\e[39m
    #####################"
  permissions: '755'

- path: /tmp/start-services.sh
  content: |
    #!/bin/bash

    set -ex
    MASTERIP=`hostname -I | col1`


    echo "Launch HADOOP starts."
    echo 'Y' | /home/hduser/hadoop/bin/hdfs namenode -format hdfs_cluster
    /home/hduser/hadoop/sbin/hadoop-daemon.sh start namenode
    echo "Launch HADOOP finished."


    echo -e "###################
    \e[92mServices STARTED!!!\e[39m
    ###################"
  permissions: '755'

- path: /tmp/hadoop/configs/hdfs-site.xml
  content: |
   <configuration>
    <property>
     <name>dfs.namenode.http-address</name>
      <value>HadoopMaster:50070</value>
    </property>
    <property>
      <name>dfs.name.dir</name>
      <value>/tmp</value>
      <final>true</final>
    </property>
    <property>
       <name>dfs.permissions</name>
       <value>false</value>
    </property>
    <property>
      <name>dfs.datanode.du.reserved</name>
      <value>500000000</value>
    </property>
    <property>
      <name>dfs.hosts.exclude</name>
      <value>/home/hduser/hadoop/etc/hadoop/dfs.exclude</value>
    </property>
    <property>
      <name>dfs.client.use.datanode.hostname</name>
      <value>true</value>
    </property>
    <property>
      <name>dfs.datanode.use.datanode.hostname</name>
      <value>true</value>
    </property>
    <property>
      <name>dfs.namenode.datanode.registration.ip-hostname-check</name>
      <value>false</value>
    </property>
   </configuration>
  permissions: '644'

- path: /tmp/hadoop/configs/core-site.xml
  content: |
      <configuration>
      <property>
        <name>fs.default.name</name>
        <value>hdfs://HadoopMaster:9000</value>
      </property>
      </configuration>
  permissions: '644'

- path: /tmp/hadoop/webconfigs/web.xml
  content: |
    <?xml version="1.0" encoding="UTF-8"?>
    <web-app version="2.4" xmlns="http://java.sun.com/xml/ns/j2ee">
      <security-constraint>
          <web-resource-collection>
              <web-resource-name>Protected</web-resource-name>
              <url-pattern>/*</url-pattern>
          </web-resource-collection>
          <auth-constraint>
              <role-name>admin</role-name>
          </auth-constraint>
      </security-constraint>
      <login-config>
          <auth-method>BASIC</auth-method>
          <realm-name>realm</realm-name>
      </login-config>
    </web-app>
  permissions: '644'

- path: /tmp/hadoop/webconfigs/jetty-web.xml
  content: |
    <Configure class="org.mortbay.jetty.webapp.WebAppContext">
     <Get name="securityHandler">
      <Set name="userRealm">
        <New class="org.mortbay.jetty.security.HashUserRealm">
          <Set name="name">realm</Set>
          <Set name="config">/home/hduser/hadoop/etc/hadoop/realm.properties</Set>
        </New>
      </Set>
     </Get>
    </Configure>
  permissions: '644'

- path: /home/hduser/consul/hosts.ctmpl
  content: |
    127.0.0.1       localhost

    # The following lines are desirable for IPv6 capable hosts
    ::1     localhost ip6-localhost ip6-loopback
    ff02::1 ip6-allnodes
    ff02::2 ip6-allrouters

    # Consul managed
    {% raw %}
    {{range service "hadoop"}}
    {{.Address}} {{.Node}}{{end}}
    {% endraw %}
  permissions: '644'

- path: /home/hduser/consul/service.json
  content: |
    {"service": {"name": "hadoop"}}
  permissions: '644'

- path: /etc/systemd/system/consul.service
  content: |
    [Unit]
    Description=consul agent
    Requires=network-online.target
    After=network-online.target

    [Service]
    Restart=on-failure
    ExecStart=/bin/bash -c "/home/hduser/consul/consul agent -server -ui -bootstrap-expect 1 -data-dir=/home/hduser/consul/data -config-file=/home/hduser/consul/service.json -bind=$(hostname -I | col1) -client=$(hostname -I | col1) >> /home/hduser/consul/logs/consul.log 2>&1"
    ExecReload=/bin/kill -HUP $MAINPID
    KillSignal=SIGTERM

    [Install]
    WantedBy=multi-user.target
  permissions: '644'

- path: /etc/systemd/system/consul-template-hosts.service
  content: |
    [Unit]
    Description=consul for hosts file
    Requires=network-online.target
    After=network-online.target

    [Service]
    Restart=on-failure
    ExecStart=/bin/bash -c "/home/hduser/consul/consul-template -consul-addr $(hostname -I | col1):8500 -template \"/home/hduser/consul/hosts.ctmpl:/etc/hosts:/tmp/downscale.sh\" >> /home/hduser/consul/logs/consul-template-hosts.log 2>&1"
    ExecReload=/bin/kill -HUP $MAINPID
    KillSignal=SIGTERM

    [Install]
    WantedBy=multi-user.target
  permissions: '644'

- path: /tmp/downscale.sh
  content: |
    #!/bin/bash

    set -e
    FILE_LOCATION=/home/hduser/hosts
    LOG_LOCATION=/home/hduser/downscale.log

    exec 3>&1 4>&2
    trap 'exec 2>&4 1>&3' 0 1 2 3
    exec 1>>$LOG_LOCATION 2>&1

    if [ ! -f $FILE_LOCATION ]; then
        echo -e `date +%Y-%m-%d` `date +"%T"` "$FILE_LOCATION File does not exist... Creating.."
        cp /etc/hosts $FILE_LOCATION
        echo -e `date +%Y-%m-%d` `date +"%T"` "$FILE_LOCATION File created!"
    fi

    if [ `diff /etc/hosts $FILE_LOCATION | grep '>' | wc -l` -gt 0 ]; then
        echo -e `date +%Y-%m-%d` `date +"%T"` "Downscale detected. Restarting name node service..."
        su - hduser -c '/home/hduser/hadoop/sbin/hadoop-daemon.sh stop namenode'
        su - hduser -c '/home/hduser/hadoop/sbin/hadoop-daemon.sh start namenode'
        echo -e `date +%Y-%m-%d` `date +"%T"` "Namenode restarted!"
        cp /etc/hosts $FILE_LOCATION
    else [ `diff /etc/hosts $FILE_LOCATION | grep '<' | wc -l` -gt 0 ]
        echo -e `date +%Y-%m-%d` `date +"%T"` "Upscaling detected. Copy template file..."
        cp /etc/hosts $FILE_LOCATION
    fi
  permissions: '644'

- path: /home/hduser/WordCount.java
  content: |
    import java.io.IOException;
    import java.util.StringTokenizer;

    import org.apache.hadoop.conf.Configuration;
    import org.apache.hadoop.fs.Path;
    import org.apache.hadoop.io.IntWritable;
    import org.apache.hadoop.io.Text;
    import org.apache.hadoop.mapreduce.Job;
    import org.apache.hadoop.mapreduce.Mapper;
    import org.apache.hadoop.mapreduce.Reducer;
    import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;
    import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;

    public class WordCount {

      public static class TokenizerMapper
          extends Mapper<Object, Text, Text, IntWritable>{

        private final static IntWritable one = new IntWritable(1);
        private Text word = new Text();

        public void map(Object key, Text value, Context context
                        ) throws IOException, InterruptedException {
          StringTokenizer itr = new StringTokenizer(value.toString());
          while (itr.hasMoreTokens()) {
            word.set(itr.nextToken());
            context.write(word, one);
          }
        }
      }

      public static class IntSumReducer
          extends Reducer<Text,IntWritable,Text,IntWritable> {
        private IntWritable result = new IntWritable();

        public void reduce(Text key, Iterable<IntWritable> values,
                          Context context
                          ) throws IOException, InterruptedException {
          int sum = 0;
          for (IntWritable val : values) {
            sum += val.get();
          }
          result.set(sum);
          context.write(key, result);
        }
      }

      public static void main(String[] args) throws Exception {
        Configuration conf = new Configuration();
        Job job = Job.getInstance(conf, "word count");
        job.setJarByClass(WordCount.class);
        job.setMapperClass(TokenizerMapper.class);
        job.setCombinerClass(IntSumReducer.class);
        job.setReducerClass(IntSumReducer.class);
        job.setOutputKeyClass(Text.class);
        job.setOutputValueClass(IntWritable.class);
        FileInputFormat.addInputPath(job, new Path(args[0]));
        FileOutputFormat.setOutputPath(job, new Path(args[1]));
        System.exit(job.waitForCompletion(true) ? 0 : 1);
      }
    }
  permissions: '644'


runcmd:
- /tmp/installation.sh && /tmp/configuration.sh && su - hduser -c '/tmp/start-services.sh' && echo "HADOOP MASTER DEPLOYMENT DONE." || echo -e "\e[91mPROBLEM OCCURED WITH THE INSTALLATION\e[39m"
