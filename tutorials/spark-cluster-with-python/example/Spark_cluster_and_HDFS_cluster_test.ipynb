{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"sztaki_logo.jpg\" height=\"112\" width=\"400\" align=\"left\"><br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Spark Cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import python libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext, SparkConf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "SparkMasterIP=\"xxxSPARKMASTERIPxxx\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start Spark Application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start Spark local mode\n",
    "# sc = SparkContext(appName=\"test\", master=\"local\")\n",
    "\n",
    "# Start Spark cluster mode\n",
    "\n",
    "sc = SparkContext(appName=\"test\", master=\"spark://\"+SparkMasterIP+\":7077\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://spark-master:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.0.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>spark://192.168.10.74:7077</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>test</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=spark://192.168.10.74:7077 appName=test>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from random import random\n",
    "from operator import add\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession\\\n",
    "    .builder\\\n",
    "    .master(\"spark://\"+SparkMasterIP+\":7077\")\\\n",
    "    .appName(\"PythonPi\")\\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://spark-master:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.0.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>spark://192.168.10.74:7077</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>test</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7ff45e9be5f8>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set a partition number\n",
    "partitions = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 100000 * partitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(_):\n",
    "    x = random() * 2 - 1\n",
    "    y = random() * 2 - 1\n",
    "    return 1 if x ** 2 + y ** 2 <= 1 else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pi is roughly 3.140460\n"
     ]
    }
   ],
   "source": [
    "count = spark.sparkContext.parallelize(range(1, n + 1), partitions).map(f).reduce(add)\n",
    "print(\"Pi is roughly %f\" % (4.0 * count / n))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ohter python libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import Row\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create an RDD and fill in a series of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(name='Steve', age=40, id=1),\n",
       " Row(name='Lui', age=10, id=2),\n",
       " Row(name='Mike', age=99, id=3)]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd = sc.parallelize(\\\n",
    "    [Row(name='Steve', age=40, id=1),\\\n",
    "     Row(name='Lui', age=10, id=2),\\\n",
    "     Row(name='Mike', age=99, id=3)\\\n",
    "    ]\\\n",
    "    , numSlices=3\n",
    ")\n",
    "rdd.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a DataFrame from RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataFrame = rdd.toDF()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data in RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+---+\n",
      "| name|age| id|\n",
      "+-----+---+---+\n",
      "|Steve| 40|  1|\n",
      "|  Lui| 10|  2|\n",
      "| Mike| 99|  3|\n",
      "+-----+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rdd.toDF().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data in DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+---+\n",
      "| name|age| id|\n",
      "+-----+---+---+\n",
      "|Steve| 40|  1|\n",
      "|  Lui| 10|  2|\n",
      "| Mike| 99|  3|\n",
      "+-----+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dataFrame.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get the number of Partitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataFrame.rdd.getNumPartitions()\n",
    "\n",
    "# must be 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stop Spark Application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.stop()\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test HDFS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2021-02-17 19:13:25--  https://raw.githubusercontent.com/occopus/docs/master/sphinx/source/tutorial-bigdata-ai.rst\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.109.133, 185.199.111.133, 185.199.110.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.109.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 47380 (46K) [text/plain]\n",
      "Saving to: ‘/home/sparkuser/text.txt’\n",
      "\n",
      "/home/sparkuser/tex 100%[===================>]  46.27K  --.-KB/s    in 0.004s  \n",
      "\n",
      "2021-02-17 19:13:25 (10.6 MB/s) - ‘/home/sparkuser/text.txt’ saved [47380/47380]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://raw.githubusercontent.com/occopus/docs/master/sphinx/source/tutorial-bigdata-ai.rst -O /home/sparkuser/text.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "!/home/sparkuser/hadoop/bin/hdfs dfs -put /home/sparkuser/text.txt /"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = SparkContext(appName=\"test\", master=\"spark://\"+SparkMasterIP+\":7077\")\n",
    "distFile = sc.textFile(\"hdfs://\"+SparkMasterIP+\":9000/text.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nonempty lines 484\n"
     ]
    }
   ],
   "source": [
    "nonempty_lines = distFile.filter(lambda x: len(x) > 0)\n",
    "print('Nonempty lines', nonempty_lines.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = nonempty_lines.flatMap(lambda x: x.split(' '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 100 words:\n",
      "[(2311, ''), (434, 'the'), (142, 'and'), (115, 'to'), (115, 'of'), (96, 'a'), (90, 'for'), (88, 'is'), (81, 'Spark'), (78, '..'), (69, '-'), (63, 'you'), (58, 'in'), (56, 'can'), (56, '#.'), (48, 'with'), (48, 'on'), (43, '|'), (40, 'your'), (37, 'be'), (36, 'node'), (33, 'infrastructure'), (32, 'nodes'), (31, 'The'), (30, 'plugin'), (30, 'cloud'), (30, 'resource'), (30, 'code::'), (27, 'by'), (26, 'port'), (26, 'this'), (26, 'are'), (26, 'sure'), (25, 'using'), (24, 'authentication'), (24, 'You'), (24, 'bash'), (23, 'Jupyter'), (23, 'we'), (22, 'important::'), (22, 'an'), (22, 'note::'), (21, 'cluster'), (21, 'Occopus'), (21, 'set'), (21, 'machine'), (20, 'information'), (20, 'that'), (20, 'or'), (20, 'may'), (19, 'at'), (18, 'Make'), (18, 'file'), (18, 'from'), (18, 'through'), (18, 'Apache'), (18, 'identifier'), (17, 'which'), (17, 'interface'), (17, 'contains'), (17, 'use'), (16, 'data'), (16, 'up'), (16, 'not'), (16, 'must'), (15, 'number'), (15, 'It'), (14, '+-------+------------------------------------------------------------------+'), (14, 'Master'), (14, 'following'), (13, 'UI'), (13, 'For'), (13, 'web'), (13, 'Keras'), (13, 'In'), (13, 'example'), (13, '.'), (13, 'TCP'), (12, 'will'), (12, 'definitions'), (12, 'any'), (12, '192.168.xxx.xxx'), (12, '14032858-d628-40a2-b611-71381bd463fa'), (12, 'application'), (12, 'nova'), (12, 'find'), (12, 'values'), (12, 'contextualisation'), (12, 'Occopus-compatible'), (12, 'tutorials'), (12, 'attributes'), (12, 'template'), (12, '==========='), (12, '============='), (12, '===================='), (11, 'Worker'), (11, 'so'), (11, 'Notebook'), (11, 'tutorial'), (11, 'JupyterLab')]\n"
     ]
    }
   ],
   "source": [
    "wordcounts = words.map(lambda x: (x, 1)) \\\n",
    "                  .reduceByKey(lambda x, y: x+y) \\\n",
    "                  .map(lambda x: (x[1], x[0])).sortByKey(False)\n",
    "print('Top 100 words:')\n",
    "print(wordcounts.take(100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.stop()\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
