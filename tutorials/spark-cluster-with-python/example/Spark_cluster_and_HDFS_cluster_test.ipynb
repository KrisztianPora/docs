{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"sztaki_logo.jpg\" height=\"112\" width=\"400\" align=\"left\"><br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Spark Cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import python libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext, SparkConf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "SparkMasterIP=\"xxxSPARKMASTERIPxxx\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start Spark Application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start Spark local mode\n",
    "# sc = SparkContext(appName=\"test\", master=\"local\")\n",
    "\n",
    "# Start Spark cluster mode\n",
    "\n",
    "sc = SparkContext(appName=\"test\", master=\"spark://\"+SparkMasterIP+\":7077\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "%pylab inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check Spark Context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://spark-master:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.4.6</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>spark://193.224.59.248:7077</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>test</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=spark://193.224.59.248:7077 appName=test>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ohter python libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import Row\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create an RDD and fill in a series of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd = sc.parallelize(\\\n",
    "    [Row(name='Steve', age=40, id=1),\\\n",
    "     Row(name='Lui', age=10, id=2),\\\n",
    "     Row(name='Mike', age=99, id=3)\\\n",
    "    ]\\\n",
    "    , numSlices=3\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unit test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test that the number of rows in resilient distributed dateset is equal 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert rdd.count() == 3, \"Should be 3\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unit test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test that rdd object hasn't got a methond named toDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert hasattr(rdd, \"toDF\") == False, \"Should be False\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hasattr(rdd, \"toDF\")\n",
    "## False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Spark Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert hasattr(rdd, \"toDF\") == True, \"Should be True\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hasattr(rdd, \"toDF\")\n",
    "## True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a DataFrame from RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataFrame = rdd.toDF()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data in RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+-----+\n",
      "|age| id| name|\n",
      "+---+---+-----+\n",
      "| 40|  1|Steve|\n",
      "| 10|  2|  Lui|\n",
      "| 99|  3| Mike|\n",
      "+---+---+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rdd.toDF().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data in DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+-----+\n",
      "|age| id| name|\n",
      "+---+---+-----+\n",
      "| 40|  1|Steve|\n",
      "| 10|  2|  Lui|\n",
      "| 99|  3| Mike|\n",
      "+---+---+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dataFrame.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get the number of Partitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataFrame.rdd.getNumPartitions()\n",
    "\n",
    "# must be 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "if( dataFrame.first().index('Steve') == 2):\n",
    "    Test = \"Spark and Python program successfully completed.\"\n",
    "else:\n",
    "    Test = \"Something goes wrong.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stop Spark Application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.stop()\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark and Python program successfully completed.\n"
     ]
    }
   ],
   "source": [
    "print(Test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test HDFS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2020-08-18 11:08:48--  https://raw.githubusercontent.com/occopus/docs/master/sphinx/source/tutorial-bigdata-ai.rst\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.192.133, 151.101.128.133, 151.101.64.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.192.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 44068 (43K) [text/plain]\n",
      "Saving to: ‘/home/sparkuser/text.txt’\n",
      "\n",
      "/home/sparkuser/tex 100%[===================>]  43.04K  --.-KB/s    in 0.009s  \n",
      "\n",
      "2020-08-18 11:08:48 (4.75 MB/s) - ‘/home/sparkuser/text.txt’ saved [44068/44068]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://raw.githubusercontent.com/occopus/docs/master/sphinx/source/tutorial-bigdata-ai.rst -O /home/sparkuser/text.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "!/home/sparkuser/hadoop/bin/hdfs dfs -put /home/sparkuser/text.txt /"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = SparkContext(appName=\"test\", master=\"spark://\"+SparkMasterIP+\":7077\")\n",
    "distFile = sc.textFile(\"hdfs://\"+SparkMasterIP+\":9000/text.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nonempty lines 475\n"
     ]
    }
   ],
   "source": [
    "nonempty_lines = distFile.filter(lambda x: len(x) > 0)\n",
    "print('Nonempty lines', nonempty_lines.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = nonempty_lines.flatMap(lambda x: x.split(' '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 100 words:\n",
      "[(2087, ''), (421, 'the'), (123, 'and'), (111, 'of'), (101, 'to'), (98, 'for'), (85, 'Spark'), (84, 'a'), (81, 'is'), (78, '..'), (71, '-'), (63, 'in'), (62, 'you'), (58, 'can'), (57, '#.'), (43, 'with'), (42, 'on'), (40, 'your'), (39, 'node'), (38, 'nodes'), (35, 'be'), (34, 'infrastructure'), (32, 'cloud'), (32, 'code::'), (30, 'resource'), (30, 'port'), (30, 'plugin'), (28, 'this'), (28, 'sure'), (26, 'The'), (26, 'by'), (25, 'You'), (25, 'bash'), (24, 'are'), (24, 'TCP'), (24, 'authentication'), (23, 'using'), (23, 'we'), (23, 'set'), (22, 'an'), (22, 'Occopus'), (22, 'important::'), (21, 'Apache'), (21, 'or'), (20, 'note::'), (20, 'may'), (20, 'cluster'), (19, 'through'), (19, 'number'), (19, 'Make'), (19, 'information'), (18, 'It'), (18, 'must'), (18, '==========='), (18, '============='), (18, '===================='), (18, 'at'), (18, 'identifier'), (18, 'contains'), (18, 'which'), (18, 'file'), (17, 'use'), (17, 'machine'), (16, '.'), (16, 'following'), (16, 'up'), (16, 'Master'), (16, 'from'), (16, 'Jupyter'), (15, 'not'), (14, 'data'), (14, '192.168.xxx.xxx'), (13, 'example'), (13, 'In'), (13, 'will'), (13, 'find'), (13, 'Keras'), (12, 'contextualisation'), (12, 'Occopus-compatible'), (12, 'tutorials'), (12, 'attributes'), (12, 'template'), (12, '(e.g.'), (12, 'Hadoop'), (12, 'scaling'), (12, 'interface'), (12, 'nova'), (12, 'any'), (12, 'values'), (12, 'so'), (12, 'implement'), (12, 'opening'), (12, 'For'), (12, 'that'), (12, 'definitions'), (12, '14032858-d628-40a2-b611-71381bd463fa'), (12, 'Worker'), (11, 'Do'), (11, 'make'), (11, 'tutorial')]\n"
     ]
    }
   ],
   "source": [
    "wordcounts = words.map(lambda x: (x, 1)) \\\n",
    "                  .reduceByKey(lambda x, y: x+y) \\\n",
    "                  .map(lambda x: (x[1], x[0])).sortByKey(False)\n",
    "print('Top 100 words:')\n",
    "print(wordcounts.take(100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.stop()\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
