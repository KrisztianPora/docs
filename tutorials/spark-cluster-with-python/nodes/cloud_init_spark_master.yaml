#cloud-config
package_upgrade: false


write_files:

- path: /tmp/installation.sh
  content: |
    #!/bin/bash

    # chage -d 2020-08-04 ubuntu
    set -ex
    HADOOP_VERSION={{variables.HADOOP_VERSION}}
    SPARK_VERSION={{variables.SPARK_VERSION}}
    SPARK_HADOOP_VERSION={{variables.SPARK_HADOOP_VERSION}}
    CONSUL_VERSION={{variables.CONSUL_VERSION}}
    CONSUL_TEMPLATE_VERSION={{variables.CONSUL_TEMPLATE_VERSION}}

    echo "Creating SPARKUSER starts."
    adduser --disabled-password --gecos "" sparkuser
    chown -R sparkuser:sparkuser /home/sparkuser
    echo "Creating SPARKUSER finished."


    hostnamectl set-hostname spark-master


    # Turn off unattended upgrade
    sed -i 's/APT::Periodic::Unattended-Upgrade "1";/APT::Periodic::Unattended-Upgrade "0";/g' /etc/apt/apt.conf.d/20auto-upgrades


    echo "Install requirement packages starts."
    # Wait for unattended upgrade
    while [[ `ps aufx | grep -v "grep" | grep "apt.systemd.daily" | wc -l` -gt 0 ]]; do
      echo "The unattended-upgrades are running..."
      sleep 1
    done

    export DEBIAN_FRONTEND=noninteractive
    apt-get update
    apt-get install -yq openjdk-8-jdk openjdk-8-jre python3-pip unzip
    su - sparkuser -c 'pip3 install pyspark==3.0.1 notebook==6.2.0 jupyter-contrib-nbextensions==0.5.1 matplotlib==3.3.4'
    echo "Install requirement packages starts."


    echo "Install HADOOP starts."
    wget -nc https://archive.apache.org/dist/hadoop/common/hadoop-$HADOOP_VERSION/hadoop-$HADOOP_VERSION.tar.gz -O /home/sparkuser/hadoop-$HADOOP_VERSION.tar.gz
    tar -xzf /home/sparkuser/hadoop-$HADOOP_VERSION.tar.gz --directory /home/sparkuser
    mkdir /home/sparkuser/hadoop
    mv /home/sparkuser/hadoop-$HADOOP_VERSION/* /home/sparkuser/hadoop
    rm -r /home/sparkuser/hadoop-$HADOOP_VERSION.tar.gz /home/sparkuser/hadoop-$HADOOP_VERSION
    echo "Install HADOOP finished."


    echo "Install SPARK starts."
    wget -nc https://archive.apache.org/dist/spark/spark-$SPARK_VERSION/spark-$SPARK_VERSION-bin-hadoop$SPARK_HADOOP_VERSION.tgz -O /home/sparkuser/spark-$SPARK_VERSION-bin-hadoop$SPARK_HADOOP_VERSION.tgz
    tar -zxf /home/sparkuser/spark-$SPARK_VERSION-bin-hadoop$SPARK_HADOOP_VERSION.tgz  --directory /home/sparkuser
    mkdir /home/sparkuser/spark
    mv /home/sparkuser/spark-$SPARK_VERSION-bin-hadoop$SPARK_HADOOP_VERSION/* /home/sparkuser/spark
    rm -r /home/sparkuser/spark-$SPARK_VERSION-bin-hadoop$SPARK_HADOOP_VERSION.tgz /home/sparkuser/spark-$SPARK_VERSION-bin-hadoop$SPARK_HADOOP_VERSION
    echo "Install SPARK finished."


    echo "Install CONSUL starts."
    wget -nc "https://releases.hashicorp.com/consul/"$CONSUL_VERSION"/consul_"$CONSUL_VERSION"_linux_amd64.zip" -O /home/sparkuser/consul_"$CONSUL_VERSION"_linux_amd64.zip
    unzip -q /home/sparkuser/consul_"$CONSUL_VERSION"_linux_amd64.zip -d /home/sparkuser/consul/
    wget -nc "https://releases.hashicorp.com/consul-template/"$CONSUL_TEMPLATE_VERSION"/consul-template_"$CONSUL_TEMPLATE_VERSION"_linux_amd64.zip" -O /home/sparkuser/consul-template_"$CONSUL_TEMPLATE_VERSION"_linux_amd64.zip
    unzip -q /home/sparkuser/consul-template_"$CONSUL_TEMPLATE_VERSION"_linux_amd64.zip -d /home/sparkuser/consul/
    rm /home/sparkuser/consul_"$CONSUL_VERSION"_linux_amd64.zip /home/sparkuser/consul-template_"$CONSUL_TEMPLATE_VERSION"_linux_amd64.zip
    echo "Install CONSUL finished."


    echo -e "####################
    \e[92mInstallation DONE!!!\e[39m
    ####################"
  permissions: '755'

- path: /tmp/configuration.sh
  content: |
    #!/bin/bash

    set -ex
    MASTERIP=`hostname -I | col1`
    HOSTNAME=`hostname -s`


    echo "Configure HADOOP starts."
    touch /home/sparkuser/.bashrc
    chown sparkuser:sparkuser /home/sparkuser/.bashrc
    echo export PATH="/home/sparkuser/hadoop/bin:$PATH" >> /home/sparkuser/.bashrc
    mv /tmp/hadoop/configs/* /home/sparkuser/hadoop/etc/hadoop
    echo "spark: lpds, admin" >> /home/sparkuser/hadoop/etc/hadoop/realm.properties
    echo "export JAVA_HOME=/usr/lib/jvm/java-8-openjdk-amd64/jre" >> /home/sparkuser/hadoop/etc/hadoop/hadoop-env.sh
    echo "export HADOOP_PID_DIR=/home/sparkuser/hadoop" >> /home/sparkuser/hadoop/etc/hadoop/hadoop-env.sh
    echo "export HADOOP_LOG_DIR=/home/sparkuser/hadoop/logs" >> /home/sparkuser/hadoop/etc/hadoop/hadoop-env.sh
    mkdir /home/sparkuser/hadoop/logs
    sed -i 's/HadoopMaster/'$MASTERIP'/g' /home/sparkuser/hadoop/etc/hadoop/core-site.xml
    sed -i 's/HadoopMaster/'$MASTERIP'/g' /home/sparkuser/hadoop/etc/hadoop/hdfs-site.xml
    touch /home/sparkuser/hadoop/etc/hadoop/dfs.exclude
    echo "$MASTERIP $HOSTNAME" >> /etc/hosts
    chown -R sparkuser:sparkuser /home/sparkuser/hadoop
    echo "Configure HADOOP finished."


    echo "Configure JUPYTER NOTEBOOK starts."
    echo export PYSPARK_DRIVER_PYTHON=jupyter >> /home/sparkuser/.bashrc
    echo export PYSPARK_DRIVER_PYTHON_OPTS=notebook >> /home/sparkuser/.bashrc
    echo export PYSPARK_PYTHON=python3 >> /home/sparkuser/.bashrc
    su - sparkuser -c '/home/sparkuser/.local/bin/jupyter contrib nbextension install --user'
    su - sparkuser -c '/home/sparkuser/.local/bin/jupyter notebook --generate-config'
    echo "c = get_config()" >> /home/sparkuser/.jupyter/jupyter_notebook_config.py
    echo "c.NotebookApp.password = u'sha1:3ba4370be377:24644d24dcb81bcbd346114fcf9095ad7c1dd0ad'"  >> /home/sparkuser/.jupyter/jupyter_notebook_config.py
    echo "Configure JUPYTER NOTEBOOK finished."


    echo "Configure SPARK starts."
    cp /home/sparkuser/spark/conf/spark-env.sh.template /home/sparkuser/spark/conf/spark-env.sh
    echo export SPARK_HOME=/home/sparkuser/spark >> /home/sparkuser/.bashrc
    echo "SPARK_MASTER_HOST=$MASTERIP >> /home/sparkuser/spark/conf/spark-env.sh"
    echo "SPARK_LOCAL_IP=$MASTERIP >> /home/sparkuser/spark/conf/spark-env.sh"
    echo "SPARK_PUBLIC_DNS=$MASTERIP >> /home/sparkuser/spark/conf/spark-env.sh"
    echo "Configure SPARK ends."


    echo "Configure SPARK GUI SECURITY starts."
    wget https://gitlab.com/lpds-public/occopus-ml/raw/master/tutorials/ext_dep/spark-gui-sec.tgz?inline=false -O /home/sparkuser/spark-gui-sec.tgz
    tar -zxf /home/sparkuser/spark-gui-sec.tgz --directory /home/sparkuser
    cp /home/sparkuser/spark-gui-sec/basicAuthenticationFilter-0.0.1-SNAPSHOT.jar /home/sparkuser/spark/jars/.
    rm -r /home/sparkuser/spark-gui-sec*
    echo "spark.master.rest.enabled    false" >>/home/sparkuser/spark/conf/spark-defaults.conf
    echo "spark.ui.filters hu.sztaki.lpds.spark.authentication.BasicAuthenticationFilter" >> /home/sparkuser/spark/conf/spark-defaults.conf
    echo "spark.hu.sztaki.lpds.spark.authentication.BasicAuthenticationFilter.params username=spark,password=lpds,realm=realm" >> /home/sparkuser/spark/conf/spark-defaults.conf
    chown -R sparkuser:sparkuser /home/sparkuser/spark
    echo "Configure SPARK GUI SECURITY ends."


    su - sparkuser -c 'mkdir /home/sparkuser/consul/logs'
    su - sparkuser -c 'mkdir /home/sparkuser/consul/data'
    touch /home/sparkuser/downscale.log
    chown sparkuser:sparkuser /home/sparkuser/downscale.log


    echo "Launch CONSUL starts."
    systemctl start consul.service
    systemctl start consul-template-hosts.service
    echo "Launch CONSUL finished."


    chmod +x /tmp/downscale.sh
    while [[ `cat /etc/hosts | grep 'Consul' | wc -l` -eq 0 ]]; do
      echo "Waiting for /etc/host modification..."
      sleep 1
    done

    echo "Creating example"
    su - sparkuser -c 'mkdir /home/sparkuser/example'
    su - sparkuser -c 'wget https://raw.githubusercontent.com/occopus/docs/devel/tutorials/spark-cluster-with-python/example/Spark_cluster_and_HDFS_cluster_test.ipynb -O /home/sparkuser/example/Spark_cluster_and_HDFS_cluster_test.ipynb'
    su - sparkuser -c 'wget https://raw.githubusercontent.com/occopus/docs/devel/tutorials/spark-cluster-with-python/example/sztaki_logo.jpg -O /home/sparkuser/example/sztaki_logo.jpg'
    sed -i "s/xxxSPARKMASTERIPxxx/$MASTERIP/g" /home/sparkuser/example/Spark_cluster_and_HDFS_cluster_test.ipynb
    echo "Example created"


    echo -e "#####################
    \e[92mConfiguration DONE!!!\e[39m
    #####################"
  permissions: '755'

- path: /tmp/start-services.sh
  content: |
    #!/bin/bash

    set -ex
    MASTERIP=`hostname -I | col1`


    echo "Launch HADOOP starts."
    echo 'Y' | /home/sparkuser/hadoop/bin/hdfs namenode -format hdfs_cluster
    /home/sparkuser/hadoop/bin/hdfs --daemon start namenode
    echo "Launch HADOOP finished."


    echo "Launch Jupyter starts."
    export PYSPARK_DRIVER_PYTHON=jupyter
    export PYSPARK_DRIVER_PYTHON_OPTS=notebook
    export PYSPARK_PYTHON=python3
    /home/sparkuser/.local/bin/jupyter notebook --ip=$MASTERIP --port=8888 >> /home/sparkuser/jupyter.log 2>&1 &
    echo "Launch Jupyter finished."


    echo "Launch Spark starts."
    export SPARK_HOME=/home/sparkuser/spark
    /home/sparkuser/spark/sbin/start-master.sh
    echo "Launch Spark finished."


    echo -e "###################
    \e[92mServices STARTED!!!\e[39m
    ###################"
  permissions: '755'

- path: /tmp/hadoop/configs/hdfs-site.xml
  content: |
   <configuration>
    <property>
     <name>dfs.namenode.http-address</name>
      <value>HadoopMaster:9870</value>
    </property>
    <property>
       <name>dfs.permissions.enabled</name>
       <value>false</value>
    </property>
    <property>
      <name>dfs.datanode.du.reserved</name>
      <value>500000000</value>
    </property>
    <property>
      <name>dfs.hosts.exclude</name>
      <value>/home/sparkuser/hadoop/etc/hadoop/dfs.exclude</value>
    </property>
    <property>
      <name>dfs.client.use.datanode.hostname</name>
      <value>true</value>
    </property>
    <property>
      <name>dfs.datanode.use.datanode.hostname</name>
      <value>true</value>
    </property>
    <property>
      <name>dfs.namenode.datanode.registration.ip-hostname-check</name>
      <value>false</value>
    </property>
   </configuration>
  permissions: '644'

- path: /tmp/hadoop/configs/core-site.xml
  content: |
      <configuration>
      <property>
        <name>fs.defaultFS</name>
        <value>hdfs://HadoopMaster:9000</value>
      </property>
      </configuration>
  permissions: '644'

- path: /home/sparkuser/consul/hosts.ctmpl
  content: |
    127.0.0.1       localhost

    # The following lines are desirable for IPv6 capable hosts
    ::1     localhost ip6-localhost ip6-loopback
    ff02::1 ip6-allnodes
    ff02::2 ip6-allrouters

    # Consul managed
    {% raw %}
    {{range service "hadoop"}}
    {{.Address}} {{.Node}}{{end}}
    {% endraw %}
  permissions: '644'

- path: /home/sparkuser/consul/service.json
  content: |
    {"service": {"name": "hadoop"}}
  permissions: '644'

- path: /etc/systemd/system/consul.service
  content: |
    [Unit]
    Description=consul agent
    Requires=network-online.target
    After=network-online.target

    [Service]
    Restart=on-failure
    ExecStart=/bin/bash -c "/home/sparkuser/consul/consul agent -server -ui -bootstrap-expect 1 -data-dir=/home/sparkuser/consul/data -config-file=/home/sparkuser/consul/service.json -bind=$(hostname -I | col1) -client=$(hostname -I | col1) >> /home/sparkuser/consul/logs/consul.log 2>&1"
    ExecReload=/bin/kill -HUP $MAINPID
    KillSignal=SIGTERM

    [Install]
    WantedBy=multi-user.target
  permissions: '644'

- path: /etc/systemd/system/consul-template-hosts.service
  content: |
    [Unit]
    Description=consul for hosts file
    Requires=network-online.target
    After=network-online.target

    [Service]
    Restart=on-failure
    ExecStart=/bin/bash -c "/home/sparkuser/consul/consul-template -consul-addr $(hostname -I | col1):8500 -template \"/home/sparkuser/consul/hosts.ctmpl:/etc/hosts:/tmp/downscale.sh\" >> /home/sparkuser/consul/logs/consul-template-hosts.log 2>&1"
    ExecReload=/bin/kill -HUP $MAINPID
    KillSignal=SIGTERM

    [Install]
    WantedBy=multi-user.target
  permissions: '644'

- path: /tmp/downscale.sh
  content: |
    #!/bin/bash

    set -e
    FILE_LOCATION=/home/sparkuser/hosts
    LOG_LOCATION=/home/sparkuser/downscale.log

    exec 3>&1 4>&2
    trap 'exec 2>&4 1>&3' 0 1 2 3
    exec 1>>$LOG_LOCATION 2>&1

    if [ ! -f $FILE_LOCATION ]; then
        echo -e `date +%Y-%m-%d` `date +"%T"` "$FILE_LOCATION File does not exist... Creating.."
        cp /etc/hosts $FILE_LOCATION
        echo -e `date +%Y-%m-%d` `date +"%T"` "$FILE_LOCATION File created!"
    fi

    if [ `diff /etc/hosts $FILE_LOCATION | grep '>' | wc -l` -gt 0 ]; then
        echo -e `date +%Y-%m-%d` `date +"%T"` "Downscale detected. Restarting name node service..."
        su - sparkuser -c '/home/sparkuser/hadoop/bin/hdfs --daemon stop namenode'
        su - sparkuser -c '/home/sparkuser/hadoop/bin/hdfs --daemon start namenode'
        echo -e `date +%Y-%m-%d` `date +"%T"` "Namenode restarted!"
        cp /etc/hosts $FILE_LOCATION
    else [ `diff /etc/hosts $FILE_LOCATION | grep '<' | wc -l` -gt 0 ]
        echo -e `date +%Y-%m-%d` `date +"%T"` "Upscaling detected. Copy template file..."
        cp /etc/hosts $FILE_LOCATION
    fi
  permissions: '644'


runcmd:
- /tmp/installation.sh && /tmp/configuration.sh && su - sparkuser -c '/tmp/start-services.sh' && echo "SPARK MASTER DEPLOYMENT DONE." || echo -e "\e[91mPROBLEM OCCURED WITH THE INSTALLATION\e[39m"