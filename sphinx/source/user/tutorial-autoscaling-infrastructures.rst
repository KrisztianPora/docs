.. _tutorial-autoscaling-infrastructures:

Tutorials on autoscaling infrastructures
========================================

Autoscaling-DataAvenue
~~~~~~~~~~~~~~~~~~~~~~

This tutorial aims to complement the scaling capabilities of Occopus. With this solution users can scale their application without user intervention in a predefined scaling range to guarantee that its running always with the optimum level of resources. In this infrastructure, nodes are discovered by Consul, which is a service discovery tool also providing DNS service and are monitored by Prometheus, a monitoring software. Prometheus supports alert definitions which later will help you write custom scaling events. 

In this autoscaling example we implemented a multi-layered traditional load-balancing schema. On the upper layer, there are load balancer nodes organised into a cluster and prepared for scaling. The load balancer cluster is handling the load generated by secured http transfer (https) between the client and the underlying application. The application is also organised inside a scalable cluster to distribute the load generated by serving the client requests. For demonstrating this architecture, the application has been represented by the Data Avenue service, which implements data transfer between the client and a remote storage using various protocols (http, sftp, s3, ...). For further details about the software, please visit `the Data Avenue website <https://data-avenue.eu>`_ . Finally, in the lowest layer there is a Database node required by the instances of Data Avenue to store and retrieve infromation (authentication, statistics) for their operation.

In case, this architecture fits to your need, you may replace the Data Avenue (with its Database node) and integrate your own application. As a result, you will have a multi-level load-balancing infrastructure, where both load balancer nodes and application servers are dynamically scaled up and down depending on the load the corresponding cluster has.

**Features**

 - using Prometheus to monitor nodes and create user-defined scaling events
 - using load balancers to share system load between data nodes
 - using Consul as a DNS service discovery agent
 - using data nodes running the application

**Prerequisites**

 - accessing a cloud through an Occopus-compatible interface (e.g. EC2, OCCI, Nova, etc.)
 - target cloud contains a base 14.04 ubuntu OS image with cloud-init support (image id, instance type)
 - start Occopus in Rest-API mode (# occopus-rest-service -h [Occopus ip address])

**Download**

You can download the example as `tutorial.examples.autoscaling-dataavenue <../../examples/autoscaling-dataavenue.tgz>`_ .

.. note::

   In this tutorial, we will use ec2 cloud resources (based on our ec2 tutorials in the basic tutorial section). However, feel free to use any Occopus-compatible cloud resource for the nodes - you can even use different types of resources for each node.

**Steps**

#. Edit ``nodes/node_definitions.yaml``. Configure the ``resource`` section as seen in the basic tutorials. For example, if you are using an ``ec2`` cloud, you have to set the following:

   - ``endpoint`` is an url of an EC2 interface of a cloud (e.g. `https://ec2.eu-west-1.amazonaws.com`).
   - ``regionname`` is the region name within an EC2 cloud (e.g. `eu-west-1`).
   - ``image_id`` is the image id (e.g. `ami-12345678`) on your EC2 cloud. Select an image containing a base os installation with cloud-init support!
   - ``instance_type`` is the instance type (e.g. `m1.small`) of your VM to be instantiated.
   - ``key_name``  optionally specifies the keypair (e.g. `my_ssh_keypair`) to be deployed on your VM.
   - ``security_group`` optionally specifies security settings (you can define multiple security groups in the form of a list, e.g. `sg-93d46bf7`) of your VM.
   - ``subnet_id`` optionally specifies subnet identifier (e.g. `subnet-644e1e13`) to be attached to the VM.

   For further explanation, read the :ref:`node definition's resource section <userdefinitionresourcesection>` of the User Guide.

   .. code::

     'node_def:da':
         -
             resource:
                 type: ec2
                 endpoint: replace_with_endpoint_of_ec2_interface_of_your_cloud
                 regionname: replace_with_regionname_of_your_ec2_interface
                 image_id: replace_with_id_of_your_image_on_your_target_cloud
                 instance_type: replace_with_instance_type_of_your_image_on_your_target_cloud
                 key_name: replace_with_key_name_on_your_target_cloud
                 security_group_ids:
                     -
                         replace_with_security_group_id1_on_your_target_cloud
                     -
                         replace_with_security_group_id2_on_your_target_cloud
                 subnet_id: replace_with_subnet_id_on_your_target_cloud
             ...
  

#. Optionally, edit the ``infra_as_dataavenue.yaml`` infrastructure descriptor file. Set the following attributes:

   - ``scaling`` is the interval in which the number of nodes can change (min,max). You can change da and lb nodes or leave them as they are.
   
   .. code::
      
      - &DA_cluster # Node Running your application
  	   name: da
	   type: da
  	   scaling:
 	     min: 1
 	     max: 10

   .. important::

      Keep in mind that Occopus has to start at least one node from each node type to work properly!

#. Optionally, you can edit the ``nodes/cloud_init_da.yaml`` node descriptor file. If you wish, you can replace the actually implemented Grid Data Avenue webapplication with your own one. Be careful, when modifying this example! 

   This autoscaling project scales the infrastructure over your application while you can run any application on it. You have to put your application code into the cloud_init_da.yaml file and make sure it starts automatically when the node boots up. This way every data node will run your application and load balancers will share the load between them. This solution fits to web applications serving high number of incoming http requests.

   .. note::

     For detailed explanation on cloud-init and its usage, please read `the cloud-init documentation <http://cloudinit.readthedocs.org/en/latest/topics/examples.html>`_!

#. Optionally, edit the ``nodes/cloud_init_prometheus.yaml`` node descriptor file's "Prometheus rules" section in case you want to implement new scaling rules. The actually implemented rules are working well and can be seen below.
   
	- ``{infra_id}`` is a built in Occopus variable and every alert has to implement it in their Labels!
	- ``node`` should be set to da or lb depending on which type of node the alerts should work.

   .. code::
      
       lb_cpu_utilization = 100 - (avg (rate(node_cpu{group="lb_cluster",mode="idle"}[60s])) * 100)
       da_cpu_utilization = 100 - (avg (rate(node_cpu{group="da_cluster",mode="idle"}[60s])) * 100)
 
    ALERT da_overloaded
      IF da_cpu_utilization > 50 
      FOR 1m
      LABELS {alert="overloaded", cluster="da_cluster", node="da", infra_id="{{infra_id}}"}
      ANNOTATIONS {
      summary = "DA cluster overloaded",
      description = "DA cluster average CPU/RAM/HDD utilization is overloaded"}
    ALERT da_underloaded
      IF da_cpu_utilization < 20
      FOR 2m
      LABELS {alert="underloaded", cluster="da_cluster", node="da", infra_id="{{infra_id}}"}
      ANNOTATIONS {
      summary = "DA cluster underloaded",
      description = "DA cluster average CPU/RAM/HDD utilization is underloaded"}
 		

   .. important::

      Autoscaling events (scale up, scale down) are based on Prometheus rules which act as thresholds, letâ€™s say scale up if cpu usage > 80%. In this example you can see the implementation of a cpu utilization in your da-lb cluster with some threshold values. Please, always use infra_id in you alerts as you can see below since Occopus will resolve this variable to your actual infrastructure id. If you are planning to write new alerts after you deployed your infrastructure, you can copy the same infrastructure id to the new one. Also make sure that the "node" property is set in the Labels subsection, too. For more information about Prometheus rules and alerts, please visit: https://prometheus.io/docs/alerting/rules/


#. Edit the "variables" section of the ``infra_as_dataavenue.yaml`` file. Set the following attributes:

   - ``occopus_restservice_ip`` is the ip address of the host where you will start the occopus-rest-service
   - ``occopus_restservice_port`` is the port you will bind the occopus-rest-service to
   
   .. code::
 
    occopus_restservice_ip: "127.0.0.1"
    occopus_restservice_port: "5000" 

#. Components in the infrastructure connect to each other, therefore several port ranges must be opened for the VMs executing the components. Clouds implement port opening various way (e.g. security groups for OpenStack, etc). Make sure you implement port opening in your cloud for the following port ranges:

   .. code::

      TCP 22   (ssh)
      TCP 8500 (Consul)
      TCP 9090 (Prometheus)
      TCP 8080 (Data Avenue)
      TCP 9093 (Alertmanager)
 
#. Make sure your authentication information is set correctly in your authentication file. You must set your authentication data for the ``resource`` you would like to use. Setting authentication information is described :ref:`here <authentication>`.

#. Load the node definitions into the database.

   .. important::

      Occopus takes node definitions from its database when builds up the infrastructure, so importing is necessary whenever the node definition or any imported (e.g. contextualisation) file changes!

   .. code::

      occopus-import nodes/node_definitions.yaml

#. Start Occopus in REST service mode:

   .. code::

      occopus-rest-service --host [ip address to bind the service to]

#. Start deploying the infrastructure through the Occopus service: 

   .. code::

      curl -X POST http://[Occopus ip address]:5000/infrastructures/ --data-binary @infra_da.yaml

#. To test the down-scaling mechanism scale up manually the da nodes through the occopus REST interface and after a few minutes you can observe that the newly connected nodes will be automatically removed because the underloaded alert is firing. You can also check the status of your alerts during the testing at ``[PrometheusIP]:9090/alerts``.

   .. code::

      curl -X POST http://[Occopus ip address]:5000/infrastructures/[infrastructure_id]/scaleup/da
   
   .. important::

      Depending on the cloud you are using for you virtual machines it can take a few minutes to start a new node and connect it to your infrastructure. The connected nodes are present on prometheus's Targets page.

#. To test the up-scaling mechanism put some load on the data nodes with the command below. Just select one of your LB node and generate load on it with running the command below in a few copy. After a few minutes the cluster will be overloaded, the overloaded alerts will fire in Prometheus and a new da node will be started and connected to your cluster. Also, if you stop sending files for a while, the overloaded alerts will fire in Prometheus and one (or more) of the da nodes will be shut (scaled) down.

   To query the nodes and their ip addresses, use this command:

   .. code::

      curl -X GET http://[Occopus ip]:5000/infrastructures/[infrastructure_id]
   
   Once, you have the ip of the selected LB node, generate load on it by transferring a 1GB file using the command below. Do not forget to update the placeholder!

   .. code::

      curl -k -o /dev/null -H "X-Key: 1a7e159a-ffd8-49c8-8b40-549870c70e73" -H "X-URI:https://autoscale.s3.lpds.sztaki.hu/files_for_autoscale/1GB.dat" http://[LB node ip address]/blacktop3/rest/file 

   To check the status of alerts under Prometheus during the testing, keep watching the following url in your browser:

   .. code::
 
      http://[prometheus node ip]:9090/alerts
 
   .. important::

      Depending on the cloud you are using for you virtual machines it can take a few minutes to start a new node and connect it to your infrastructure. The connected nodes are present on prometheus's Targets page.

#. Finally, you may destroy the infrastructure using the infrastructure id.

   .. code::

      curl -X DELETE http://[occopus ip address]:5000/infrastructures/[infra id]


